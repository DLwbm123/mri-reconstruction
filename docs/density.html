<!DOCTYPE html>
<html>
<meta charset="utf-8">
  <head>
    <title>Density estimation</title>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { fonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML-full">
    </script>
    <link rel="stylesheet" href="style.css">
 </head>
 <body>
   <div>
  <h1>Density estimation</h1>

  Why is density estimation hard?
  In high dimensions are the many directions and a large amount of volume.

  <h2> Maximum likelihood</h2>

  In an ideal world we would like the ability to approximate arbitrary distributions. To learn $ f: X \to Y $ such that the probability density is preserved. An easy and intuitive way to learn a density model is by maximum likelihood estimation.

  $$
  \begin{align}
  \hat \theta &= \mathop{\text{argmax}}_{\theta} E \left[ p(x, \theta), x\sim D \right] \\
  \end{align}
  $$

  Thus, models that allocates higher probability to observed $ x $s are better.
  However, the class of functions that $\theta$ can represent is limited.
  We would like to use an abitrary functon approximator like neural networks,
  but they fail at ML because they can simply predict $ nn(x_i) = \infty $
  for all inputs. They can do this because they are not normalised.

  A simple representation like a tensor that is indexed by possible $x $s has a
  similar normalisation problem (naively optimising it to do ML will give $ \infty $).
  But, it can be easily constrained/regualised to give normalised results.

  $$
  p(x) = T[x] \\
  $$

  $$
  \hat T = \mathop{\text{argmax}}_{T} \mathbb E \left[ p(x), x\sim D\right] \text{s.t.} \sum_i T_i = 1
  $$

  Which might be implemented as simply the decay of each element towards zero probabilty.
  But, just for mnist $T$ would need have $(28 \times 28)^{256}$ elements for each possible image (which according to Google's calculator is infinity...).


  In principle this idea could be applied to neural networks as well.

  So, if we increase the probability of a location, that should decrease the probability of other locations.

  $$
  \begin{align}
  p(x) = \frac{f(x, \theta)}{\int f(x, \theta) dx} \\
  \end{align}
  $$

  Empirical estimates of $f$ when it is a NN. Hmm, not sure how to do that...

  Want to find a parameterised fn that is easily integrated. Oh, how about $e^{x}$...

  $$
  \begin{align}
  f(x) &= g_n( \dots g_1(g_0(x))) \\
  \int f(x) dx &= ?? \\
  \end{align}
  $$

  Want some sort of decomposition of the integral into something nicer.
  Want an analytical way to calculate it!?

  <h2>Normalising flows</h2>

  What if instead we could start with a simple distribution and transform it into an arnitrary distribution?

  Effectively just pushing around probability mass.
  So how can we keep tr

  $$
  p(f(x)) = \frac{1}{det(J(f))}p(x) \\
  $$

  But want to decompse the prior.
  Masked autoregressive.

  Problems.
  <ol>
    <li> Too much non linearity makes ...</li>
    <li> Must be ivertible!?</li>
  </ol>

</div>
</body>
</html>
