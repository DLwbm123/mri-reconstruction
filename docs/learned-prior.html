<!DOCTYPE html>
<html>
<meta charset="utf-8">
  <head>
    <title>MRI Reconstruction</title>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { fonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML-full">
    </script>
    <link rel="stylesheet" href="style.css">
  </head>

<body>
  <div>
  <h1>Learned priors</h1>

  Two alternatives were explored; VAEs, GANS with RIMs on the queue.

  <h2><a href=https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>GANs</a></h2>

  I explored the properties of a simple 2d GAN. The generator takes a 1D noise signal and maps it into a 2D space. The goal is to learn to generate samples from P(x). However, the infamous mode-collapse-problem is present.

  This is significant because if a GAN has dropped modes of the data then optimising $P_{gan}(x)$ will push $x$ towards only the modes captured by the GAN, and this may be at the cost of modes not captured by the GAN.

  <img src="assets/gan_init.png">
  <img src="assets/gan_collapse.png">

  *At initialisation, the gradients don't point anywhere useful and the generator has been initialised spanning one of the modes.*

  Unless the mode collapse problem can be reliably solved (with guarantees), GANs do not seem like a good idea. Note: there is a large amount of existing prior work here, see [reading.md](reading.md).

  <h2><a href=https://arxiv.org/abs/1312.6114>VAEs </a></h2>

  I started with a VAE capable of producing good samples.

  <img src="assets/gen.png">

  Then, to get an estimate of  $ P(x) $ I sampled from the posterior and used the prior to estimate its probability.
  $$
  \begin{align}
  h_i &= f(x_i) \tag{encode $x_i$} \\
  \end{align}
  $$

  $$
  \begin{align}
  p(x) &:= \mathbb E_{z \sim p(\cdot | h_i)} \left[ p_{prior}(z) \right] \tag{expected prior prob}\\
  \end{align}
  $$

  In the case of the latent space being define as a univariate gaussian, $ h_i = \mu_i, \sigma_i $ and samples are taken from $ \mathcal N(\mu_i, \sigma_i) $ and the prior is defined to be $ \mathcal N(0, 1) $.

  $$
  \begin{align}
  \hat x &= \mathop{\text{argmax}}_x p(x) \\
  x_{t+1} &= x_t + \eta \frac{\partial p(x)}{\partial x} \tag{gradient ascent}\\
  \end{align}
  $$

  So using this estimate of $ P(x) $ we can follow its density estimate towards more likely images. Thus filling is any missing information in an reconstruction.

  <img src="assets/opt_px_same.png">
  <img src="assets/same_opt_px_imgs.png">

  Despite and increase in $P(x) $, there is no obvious difference in the images. Similarly, below...

  <img src="assets/p_x.png">
  <img src="assets/init_x.png">

  <i>The $x_i$s at initialisation (initialised as MNIST digits plus noise).</i>

  <img src="assets/finals_x.png">

  <i>The $ x_i $s after 1000 steps of gradient ascent.</i>

  So the general problem seems to be that we can happily optimise P(x) (examples below), but an increase in P(x) does not necessarily to give us useful results.

  <img src="assets/opt_px_graph_low_lr.png">
  <img src="assets/opt_px_low_lr.png">

  <i>Images generated every 100 steps (left to right, top to bottom).</i>

  <img src="assets/opt_px.png">
  <img src="assets/opt_px_img.png">

  <i>Images generated every 100 steps (left to right, top to bottom).</i>

  Why doesnt this work??? Good question.

  In hindsight I am not sure this makes sense. As $f(x)$ is supposed to give an approximation to the posterior distribution, $q(z | x)$

  $$
  \begin{align}
  p(x) &= E_{ z\sim p(z \mid x) } \left[ p(z) \right] \tag{from above}\\
  &=  \sum_i p(z_i \mid x_i) p(z_i) \tag{!?} \\
  \end{align}
  $$

  Instead, we could calculate $p(x_i)$ as
  $$
  \begin{align}
  p(x_i) &= E_{ z\sim p(z) } \left[ p(x_i \mid z) \right]  \tag{likelihood of $x_i$ under our prior}\\
  \end{align}
  $$
  (doesnt seem to work either. maybe you just need more samples.)
  This is pretty much just a parzen window!?!?

  Maybe this would work with a richer output distribution, rather than just a univariate gaussian?
  (intuition) problem is that even just adding a single pixel of noise might bring $p(x_i | z)$ down to zero.

  <h3>RIMs</h3>

  The code for RIMs exists in <a href=https://github.com/act65/mri-reconstruction/tree/master/src>src</a>, but it will take some more effort to reproduce the papers results.

  <h2>Future</h2>

  There is no reason $R(x) $ must be explicitly representing ...
  Could learn a sparse representation of the data
   $ \mathop{\text{argmin}}_{\theta} \parallel d(e(x)) - x \parallel_2 + \parallel e(x) \parallel_1 $ and use this to regularise reconstructions to plausible images.

  Candidate reconstructions that are not sparse in this representation will be pushed in that direction.

</div>
</body>
</html>
