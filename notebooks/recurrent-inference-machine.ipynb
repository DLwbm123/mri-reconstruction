{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from utils import PyramidLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-9a98769c3484>:3: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From /home/act65/anaconda3/envs/mri/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/act65/anaconda3/envs/mri/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/act65/anaconda3/envs/mri/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/act65/anaconda3/envs/mri/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/act65/anaconda3/envs/mri/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/act65/anaconda3/envs/mri/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "inputs = tf.placeholder(shape=[batch_size, 784], dtype=tf.float32)\n",
    "x = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "x = tf.cast(x, tf.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex2float(x):\n",
    "    return tf.concat([tf.real(x), tf.imag(x)], axis=-1)\n",
    "\n",
    "def complex_random(init):\n",
    "    def func(shape, dtype=None, partition_info=None):\n",
    "        real = init(dtype=tf.float32, shape=shape)\n",
    "        imag = init(dtype=tf.float32, shape=shape)\n",
    "        return tf.complex(real, imag)\n",
    "    return func\n",
    "\n",
    "def mri(x):\n",
    "    y = tf.fft2d(x)\n",
    "    \n",
    "    init_fn = partial(tf.random_uniform, minval=0, maxval=1)\n",
    "    sub_sampler = complex_random(init_fn)(tf.shape(y))\n",
    "    y *= sub_sampler\n",
    "    y += complex_random(tf.random_normal)(tf.shape(y))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(50, 28, 28, 1) dtype=complex64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = mri(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Recurrent inference machines](https://openreview.net/forum?id=HkSOlP9lg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dydx(y, x, sigma=1.0):\n",
    "    # TODO want to verify this for myself.\n",
    "    z = tf.fft(x)  # TODO add mask\n",
    "    return tf.ifft2d(z-y)/(sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RIM(tf.keras.Model):\n",
    "    def __init__(self, dydx, units):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dydx (func): the gradient of y w.r.t the mri fn\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__(self)\n",
    "        self.units = units\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        \"\"\"\n",
    "        Constructs:\n",
    "            encoder (tf.Model): a parameterised model that updates the state of the RIM\n",
    "            decoder (tf.Model): a parameterised model that updates our candidate image, x.\n",
    "        \"\"\"\n",
    "        self.cnn1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 4, strides=(2, 2), padding='same'),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.selu),\n",
    "            tf.keras.layers.Conv2D(4, 4, strides=(2, 2), padding='same'),\n",
    "        ])\n",
    "        self.cnn2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, 4, strides=(2, 2), padding='same'),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.selu),\n",
    "            tf.keras.layers.Conv2D(4, 4, strides=(2, 2), padding='same'),\n",
    "        ])\n",
    "        \n",
    "        self.dcnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2DTranspose(32, 4, strides=(2, 2), padding='same'),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.selu),\n",
    "            tf.keras.layers.Conv2DTranspose(32, 4, strides=(2, 2), padding='same'),\n",
    "            tf.keras.layers.Activation(tf.keras.activations.selu),\n",
    "            tf.keras.layers.Conv2DTranspose(1, 1, strides=(1, 1), padding='same')\n",
    "        ])\n",
    "        \n",
    "        self.rnn = tf.keras.layers.RNN([\n",
    "            tf.keras.layers.GRUCell(self.units),\n",
    "            tf.keras.layers.GRUCell(self.units)\n",
    "        ])\n",
    "\n",
    "    def call(self, y, iters=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y (tf.tensor): the k-space samples\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO how is this trained with big images!? not possible!? \n",
    "        # the recurrent dependencies will use up lots of memory!!\n",
    "        \n",
    "        x_t = tf.ifft2d(y)  # init x_0\n",
    "\n",
    "#         state = tf.zeros(shape=[y.shape[0], self.units, 2], dtype=tf.float32)\n",
    "        # for loop or rnn loop or while loop?\n",
    "        \n",
    "        self.candidate_xs = [x_t]\n",
    "        \n",
    "        for i in range(iters):\n",
    "            g = dydx(y, x_t)\n",
    "\n",
    "            g_e = self.embed(g, self.cnn1)\n",
    "            x_t_e = self.embed(x_t, self.cnn2)\n",
    "            \n",
    "            h = self.encoder(g_e, x_t_e)\n",
    "            x_t += self.decoder(g_e, x_t_e, h)\n",
    "            \n",
    "            self.candidate_xs.append(x_t)\n",
    "\n",
    "        # why not just a big rnn?\n",
    "        # ah. because we want to calculate dydx for each x_t\n",
    "        # x_t = rnn(dydx!?!, ...)\n",
    "\n",
    "        return x_t\n",
    "    \n",
    "    def embed(self, x, cnn):\n",
    "        x = complex2float(x)\n",
    "        x = cnn(x)\n",
    "        self.shape = x.shape\n",
    "        return tf.reshape(x, (x.shape[0], -1))\n",
    "    \n",
    "    def encoder(self, g, x_t):\n",
    "        x = tf.concat([g, x_t], axis=-1)\n",
    "        x = tf.reshape(x, [x_t.shape[0], 1, -1])\n",
    "\n",
    "        y = self.rnn(x)\n",
    "        return y\n",
    "\n",
    "    def decoder(self, g, x_t, h, agg='add'):\n",
    "        if agg == 'add':\n",
    "            x = g + x_t + h\n",
    "            x = tf.reshape(x, self.shape)\n",
    "        elif agg == 'concat':\n",
    "            g = tf.reshape(g, self.shape)\n",
    "            x_t = tf.reshape(x_t, self.shape)\n",
    "            h = tf.reshape(h, [h.shape[0], 1, 1, h.shape[1]])\n",
    "            x = tf.concat([g, x_t], axis=-1) + h\n",
    "\n",
    "        y = self.dcnn(x)\n",
    "        y = tf.cast(y, tf.complex64)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rim = RIM(dydx, 196)\n",
    "x_t = rim(y)\n",
    "\n",
    "x, x_t = (tf.real(x), tf.real(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new idea. can introduce a loss at every step, rather than just at\n",
    "# the end. need to test versus loss at end.\n",
    "\n",
    "def summarise_candidates(model):\n",
    "    for i, x_t in enumerate(rim.candidate_xs):\n",
    "        tf.summary.image('x_t/{}'.format(i), tf.real(x_t), max_outputs=1)\n",
    "        \n",
    "def candidate_loss(model, x):\n",
    "    return tf.add_n([tf.losses.mean_squared_error(x, x_t)\n",
    "              for x_t in rim.candidate_xs])\n",
    "        \n",
    "summarise_candidates(rim)\n",
    "loss = candidate_loss(rim, x)\n",
    "tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tf.summary.image('x_t', x_t)\n",
    "# # tf.summary.image('x', x)\n",
    "\n",
    "\n",
    "# loss = tf.losses.mean_squared_error(x, x_t)\n",
    "# # loss_fn = PyramidLoss(32, 3)\n",
    "# # loss = loss_fn((x, x_t))\n",
    "# tf.summary.scalar('loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer()\n",
    "train_step = opt.minimize(loss, var_list=rim.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"/tmp/rim/candidate/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4167937636375427"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    idx = np.random.randint(0, len(mnist.train.images), 50)\n",
    "    sess.run(train_step, feed_dict={inputs: mnist.train.images[idx, ...]})\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        L, summary = sess.run([loss, merged], feed_dict={inputs: mnist.test.images[:50, ...]})\n",
    "        writer.add_summary(summary, i)\n",
    "        print('\\rloss: {}'.format(L), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
