{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the inverse\n",
    "\n",
    "There exists $f: x \\rightarrow y$, want $f^{-1}$. In all cases we get acess to y.\n",
    "\n",
    "| X  | Sparsity  | no f  | f |\n",
    "|---|---|---|---|\n",
    "| x | sparse x | dictionary learning!?   |   |\n",
    "| no x | sparse x |  clustering __X__ | compressed sensing  |\n",
    "| x |  sparse y | supervised learning (categorical) | __us__   |\n",
    "| no x | sparse y | __X__ |  also compressed sensing __?__  |\n",
    "| x |  not sparse | supervised learning (regression)  |   |\n",
    "| no x | not sparse  | unsupervised learning __X__ |   |\n",
    "\n",
    "_also a distinction between having X versus having pairs!?_\n",
    "\n",
    "__X__ indicates that $f^{-1}$ is not recoverable\n",
    "\n",
    "\n",
    "__Q__ What does having $f$, or X, or ... buy us? Want bounds on each for thair ability to efficiently learn $f^{-1}$.\n",
    "\n",
    "\n",
    "Properties of $f$\n",
    "\n",
    "| X  | Sparsity  | no noise  | noise |\n",
    "|---|---|---|---|\n",
    "| linear | invertible |   |   |\n",
    "| non-linear | invertible |   |   |\n",
    "| linear | not invertible |   |   |\n",
    "| non-linear | not invertible |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two parts. \n",
    "- __Consistency__: Those x's that are consistent with the observations, y. \n",
    "- __Search__: Of the possible x's, which ones are plausible given other assumtions and knowledge!?\n",
    "\n",
    "Ideally we would only consider consistent x images? \n",
    "- How can we ensure this?\n",
    "- How can we find the minimal set that is consistent?\n",
    "- ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uihzVlkRuHe"
   },
   "source": [
    "## Learning a better prior\n",
    "\n",
    "__Q__ How to incorporate the priors!?!? Which approach is better? What are the advantages/disadvantages?\n",
    "\n",
    "- Meta learning is used to learn how to correct the imperfect $P(x \\mid y)$\n",
    "- GANs to learn $P(x), P(y)$  \n",
    "- Distill them info a learned forward model $NN(y)$\n",
    "\n",
    "\n",
    "### Meta\n",
    "\n",
    "Meta learning needs to learn to correct $\\frac{dL}{dx}$\n",
    "\n",
    "\n",
    "### GANS\n",
    "\n",
    "But how do we learn $P(X)$? In reality we dont really have the ground truth $x$ values, only noisy samples/reconstructions from $y$.\n",
    "\n",
    "GANs add an extra loss, maybe that provides some useful info!?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "\n",
    "Given $(X, Y, f, df)$, we want $f^{-1}$.\n",
    "\n",
    "- $f$ is lossy.\n",
    "- \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x &\\neq f^{-1}(f(x)) \\tag{not invertible/reversible}\\\\\n",
    "x^* &= \\mathop{\\text{argmin}}_x d(y, f(x)) + R(x) \\tag{optimisation problem} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Point is that $d$ is an imperfect measure because of $f$ (which throws away info). So we need $R(x)$ to incorporate prior about likely values of $x$.\n",
    "\n",
    "### Derivation of ?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x \\mid y) &= \\frac{P(y \\mid x )P(x)}{P(y)} \\\\\n",
    "log P(x \\mid y) &= logP(y \\mid x ) + logP(x) - logP(y) \\\\\n",
    "x^* &= \\mathop{\\text{argmax}}_x log P(x \\mid y)\\\\\n",
    "\\\\\n",
    "-logP(y \\mid x) &= e^{\\parallel y - f(x) \\parallel} \\tag{or f(x) - y?} \\\\\n",
    "-logP(x) &= e^{\\parallel x\\parallel} \\\\\n",
    "-log P(x \\mid y) &= \\parallel y - f(x) \\parallel + \\parallel x \\parallel \\\\\n",
    "x^* &= \\mathop{\\text{argmin}}_x  \\parallel y - f(x) \\parallel + \\parallel x \\parallel\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "__TODO__ not convinced you can throw away $logP(y)$\n",
    "\n",
    "### Lossy\n",
    "\n",
    "The loss of info. This means $f: x \\rightarrow y$ is a many-to-one function.\n",
    "\n",
    "E.g. Given $x_1, x_2$, let $y_1 = f(x_1) = y_2 = f(x_2)$, thus, no function can take $y_1$ as input and recover the true input.\n",
    "\n",
    "Thus there exists no function"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "not-reversible.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
